name: Run Spark Unit Tests

on:
  push:
    branches: [ "main", "develop" ]
    paths:
      - 'spark_df_utils/**'
      - 'tests/**'
      - 'requirements*.txt'
      - 'setup.py'
      - '.github/workflows/test.yml'
  pull_request:
    branches: [ "main", "develop" ]
    paths:
      - 'spark_df_utils/**'
      - 'tests/**'
      - 'requirements*.txt'
      - 'setup.py'
      - '.github/workflows/test.yml'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]
        java-version: ["11"]
        pyspark-version: ["3.5.6"]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}

    - name: Set up Java 11
      uses: actions/setup-java@v5
      with:
        distribution: "temurin"
        java-version: ${{ matrix.java-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pyspark==${{ matrix.pyspark-version }} pytest pytest-cov pytest-spark
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Lint with flake8
      run: |
        pip install flake8
        # stop the build if there are Python syntax errors or undefined names
        flake8 spark_df_utils tests --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings
        flake8 spark_df_utils tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest
      run: |
          coverage run -m pytest  -v -s
    
    - name: Generate Coverage Report
      run: |
          coverage report -m
